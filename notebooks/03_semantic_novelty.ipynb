{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ebfc0a",
   "metadata": {},
   "source": [
    "# 03 â€” Semantic Novelty\n",
    "\n",
    "This notebook:\n",
    "1) Loads LSA embeddings from `data/lsa/chunk_embeddings.npy`\n",
    "2) Loads aligned metadata from `data/lsa/chunk_index.jsonl`\n",
    "3) Computes novelty metrics per chunk:\n",
    "   - novelty vs cumulative history centroid\n",
    "   - novelty vs trailing window centroid\n",
    "   - novelty vs previous chunk\n",
    "4) Aggregates novelty into per-document summary + novelty curve outputs\n",
    "\n",
    "Outputs:\n",
    "- `data/lsa/novelty_per_chunk.jsonl`\n",
    "- `data/lsa/novelty_summary_per_doc.jsonl`\n",
    "- `data/lsa/novelty_curves_per_doc.jsonl`\n",
    "\n",
    "Interpretation:\n",
    "- Low novelty over time suggests low semantic information rate (high predictability)\n",
    "- High novelty spikes often indicate topic shifts or introduction of new ideas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fda91eb",
   "metadata": {},
   "source": [
    "## Imports + paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a5b78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from _paths import set_repo_root\n",
    "ROOT = set_repo_root()\n",
    "\n",
    "EMB_IN = ROOT / \"data\" / \"lsa\" / \"chunk_embeddings.npy\"\n",
    "INDEX_IN = ROOT / \"data\" / \"lsa\" / \"chunk_index.jsonl\"\n",
    "\n",
    "OUT_DIR = ROOT / \"data\" / \"lsa\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "NOVELTY_CHUNKS_OUT = OUT_DIR / \"novelty_per_chunk.jsonl\"\n",
    "NOVELTY_DOC_SUMMARY_OUT = OUT_DIR / \"novelty_summary_per_doc.jsonl\"\n",
    "NOVELTY_CURVES_OUT = OUT_DIR / \"novelty_curves_per_doc.jsonl\"\n",
    "\n",
    "print(\"Embeddings:\", EMB_IN.resolve())\n",
    "print(\"Index:\", INDEX_IN.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478e356e",
   "metadata": {},
   "source": [
    "## Load embeddings + aligned metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db038ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "Z = np.load(EMB_IN)  # shape [n_chunks, d], should already be L2-normalized\n",
    "meta = read_jsonl(INDEX_IN)\n",
    "\n",
    "if len(meta) != Z.shape[0]:\n",
    "    raise ValueError(f\"Mismatch: {len(meta)} metadata rows vs {Z.shape[0]} embeddings\")\n",
    "\n",
    "print(\"Loaded:\", Z.shape[0], \"chunks with dim\", Z.shape[1])\n",
    "print(\"Example meta:\", meta[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e95363a",
   "metadata": {},
   "source": [
    "### Helper: group chunks by document (in order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build doc -> list of indices (already sorted upstream, but we'll ensure ordering by chunk_index)\n",
    "doc_to_indices: Dict[str, List[int]] = {}\n",
    "doc_info: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "for i, r in enumerate(meta):\n",
    "    doc_id = r[\"doc_id\"]\n",
    "    doc_to_indices.setdefault(doc_id, []).append(i)\n",
    "    doc_info.setdefault(doc_id, {\"title\": r[\"title\"], \"chunk_type\": r[\"chunk_type\"]})\n",
    "\n",
    "# Sort indices by chunk_index within each doc\n",
    "for doc_id, idxs in doc_to_indices.items():\n",
    "    idxs.sort(key=lambda i: meta[i][\"chunk_index\"])\n",
    "\n",
    "print(\"Documents:\", len(doc_to_indices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c200cb27",
   "metadata": {},
   "source": [
    "## Novelty definitions\n",
    "\n",
    "Because embeddings are unit-normalized, cosine similarity is just dot product.\n",
    "\n",
    "We compute three novelty signals:\n",
    "\n",
    "1) **Cumulative novelty**: novelty of chunk t vs centroid(0..t-1)\n",
    "2) **Window novelty**: novelty vs centroid(t-w..t-1)\n",
    "3) **Delta novelty**: novelty vs chunk t-1\n",
    "\n",
    "All yield values in ~[0, 2] (but typically [0, 1.2]). Lower = more predictable/redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf988df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(v: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    n = np.linalg.norm(v)\n",
    "    if n < eps:\n",
    "        return v\n",
    "    return v / n\n",
    "\n",
    "def novelty_vs_vector(z_t: np.ndarray, ref: np.ndarray) -> float:\n",
    "    # novelty = 1 - cosine_similarity; with L2-normalized vectors, cosine = dot\n",
    "    ref_n = l2_normalize(ref)\n",
    "    return float(1.0 - np.dot(z_t, ref_n))\n",
    "\n",
    "def compute_doc_novelty(\n",
    "    Z: np.ndarray,\n",
    "    indices: List[int],\n",
    "    window: int = 5,\n",
    ") -> List[Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Returns list of dicts aligned to each chunk in this doc:\n",
    "      - novelty_cum: vs centroid of all previous chunks\n",
    "      - novelty_win: vs centroid of trailing window\n",
    "      - novelty_prev: vs previous chunk\n",
    "    \"\"\"\n",
    "    out: List[Dict[str, float]] = []\n",
    "    prev_vec: Optional[np.ndarray] = None\n",
    "\n",
    "    # Maintain cumulative sum for fast centroid\n",
    "    cum_sum = np.zeros(Z.shape[1], dtype=np.float64)\n",
    "\n",
    "    # Maintain rolling window list\n",
    "    win_vecs: List[np.ndarray] = []\n",
    "\n",
    "    for t, idx in enumerate(indices):\n",
    "        z = Z[idx].astype(np.float64)\n",
    "\n",
    "        if t == 0:\n",
    "            novelty_cum = float(\"nan\")\n",
    "            novelty_win = float(\"nan\")\n",
    "            novelty_prev = float(\"nan\")\n",
    "        else:\n",
    "            # cumulative centroid is average of previous vectors\n",
    "            cum_centroid = cum_sum / t\n",
    "            novelty_cum = novelty_vs_vector(z, cum_centroid)\n",
    "\n",
    "            # window centroid: average of last `window` vectors\n",
    "            w = min(window, len(win_vecs))\n",
    "            win_centroid = np.mean(win_vecs[-w:], axis=0)\n",
    "            novelty_win = novelty_vs_vector(z, win_centroid)\n",
    "\n",
    "            # previous chunk novelty\n",
    "            novelty_prev = novelty_vs_vector(z, prev_vec)\n",
    "\n",
    "        out.append({\n",
    "            \"novelty_cum\": novelty_cum,\n",
    "            \"novelty_win\": novelty_win,\n",
    "            \"novelty_prev\": novelty_prev,\n",
    "        })\n",
    "\n",
    "        # Update state AFTER computing novelty\n",
    "        cum_sum += z\n",
    "        win_vecs.append(z)\n",
    "        prev_vec = z\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683079b6",
   "metadata": {},
   "source": [
    "## Compute novelty for all docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28e774",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 5  # trailing window size for novelty_win\n",
    "\n",
    "novelty_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for doc_id, idxs in doc_to_indices.items():\n",
    "    novs = compute_doc_novelty(Z, idxs, window=WINDOW)\n",
    "    for idx, nov in zip(idxs, novs):\n",
    "        r = meta[idx]\n",
    "        novelty_rows.append({\n",
    "            **r,\n",
    "            **nov,\n",
    "        })\n",
    "\n",
    "print(\"Novelty rows:\", len(novelty_rows))\n",
    "print(\"Example novelty row:\", {k: novelty_rows[0][k] for k in [\"title\",\"chunk_index\",\"novelty_cum\",\"novelty_win\",\"novelty_prev\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a31aeb",
   "metadata": {},
   "source": [
    "## Summaries per document\n",
    "\n",
    "We compute stable summary stats excluding the first chunk (which is NaN by definition)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aef336c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite(vals: List[float]) -> np.ndarray:\n",
    "    arr = np.array(vals, dtype=np.float64)\n",
    "    return arr[np.isfinite(arr)]\n",
    "\n",
    "def summarize(values: np.ndarray) -> Dict[str, float]:\n",
    "    if values.size == 0:\n",
    "        return {\"mean\": float(\"nan\"), \"median\": float(\"nan\"), \"p10\": float(\"nan\"), \"p90\": float(\"nan\")}\n",
    "    return {\n",
    "        \"mean\": float(np.mean(values)),\n",
    "        \"median\": float(np.median(values)),\n",
    "        \"p10\": float(np.percentile(values, 10)),\n",
    "        \"p90\": float(np.percentile(values, 90)),\n",
    "    }\n",
    "\n",
    "doc_summaries: List[Dict[str, Any]] = []\n",
    "doc_curves: List[Dict[str, Any]] = []\n",
    "\n",
    "for doc_id, idxs in doc_to_indices.items():\n",
    "    rows = [r for r in novelty_rows if r[\"doc_id\"] == doc_id]\n",
    "    rows = sorted(rows, key=lambda r: r[\"chunk_index\"])\n",
    "\n",
    "    nov_cum = finite([r[\"novelty_cum\"] for r in rows])\n",
    "    nov_win = finite([r[\"novelty_win\"] for r in rows])\n",
    "    nov_prev = finite([r[\"novelty_prev\"] for r in rows])\n",
    "\n",
    "    title = doc_info[doc_id][\"title\"]\n",
    "    chunk_type = doc_info[doc_id][\"chunk_type\"]\n",
    "    n_chunks = len(rows)\n",
    "\n",
    "    doc_summaries.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"title\": title,\n",
    "        \"chunk_type\": chunk_type,\n",
    "        \"n_chunks\": n_chunks,\n",
    "        \"window\": WINDOW,\n",
    "        \"novelty_cum\": summarize(nov_cum),\n",
    "        \"novelty_win\": summarize(nov_win),\n",
    "        \"novelty_prev\": summarize(nov_prev),\n",
    "    })\n",
    "\n",
    "    # Save full curve values for plotting later\n",
    "    doc_curves.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"title\": title,\n",
    "        \"chunk_type\": chunk_type,\n",
    "        \"window\": WINDOW,\n",
    "        \"curve\": [\n",
    "            {\n",
    "                \"chunk_index\": int(r[\"chunk_index\"]),\n",
    "                \"novelty_cum\": (None if not np.isfinite(r[\"novelty_cum\"]) else float(r[\"novelty_cum\"])),\n",
    "                \"novelty_win\": (None if not np.isfinite(r[\"novelty_win\"]) else float(r[\"novelty_win\"])),\n",
    "                \"novelty_prev\": (None if not np.isfinite(r[\"novelty_prev\"]) else float(r[\"novelty_prev\"])),\n",
    "            }\n",
    "            for r in rows\n",
    "        ],\n",
    "    })\n",
    "\n",
    "print(\"Doc summaries:\", len(doc_summaries))\n",
    "print(\"Example summary:\", doc_summaries[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd2c877",
   "metadata": {},
   "source": [
    "## Save outputs (JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122f08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_jsonl(NOVELTY_CHUNKS_OUT, novelty_rows)\n",
    "write_jsonl(NOVELTY_DOC_SUMMARY_OUT, doc_summaries)\n",
    "write_jsonl(NOVELTY_CURVES_OUT, doc_curves)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\"-\", NOVELTY_CHUNKS_OUT, f\"({NOVELTY_CHUNKS_OUT.stat().st_size} bytes)\")\n",
    "print(\"-\", NOVELTY_DOC_SUMMARY_OUT, f\"({NOVELTY_DOC_SUMMARY_OUT.stat().st_size} bytes)\")\n",
    "print(\"-\", NOVELTY_CURVES_OUT, f\"({NOVELTY_CURVES_OUT.stat().st_size} bytes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077697bb",
   "metadata": {},
   "source": [
    "## Fun diagnostic: most/least novel chunks per document\n",
    "\n",
    "This is where you get \"semantic entropy collapse\" examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b10ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: load the raw cleaned chunk text for preview (if you want it here)\n",
    "# The index jsonl doesn't include text (by design). If you want previews, re-load chunks.jsonl.\n",
    "CHUNKS_TEXT_IN = ROOT / \"data\" / \"texts_clean\" / \"chunks.jsonl\"\n",
    "chunks_with_text = read_jsonl(CHUNKS_TEXT_IN)\n",
    "\n",
    "# Build lookup: chunk_id -> text (and title/index for sanity)\n",
    "text_lookup = {r[\"chunk_id\"]: r[\"text\"] for r in chunks_with_text}\n",
    "\n",
    "def preview_chunk(chunk_id: str, n: int = 240) -> str:\n",
    "    t = text_lookup.get(chunk_id, \"\")\n",
    "    t = t.replace(\"\\n\", \" \")\n",
    "    return t[:n] + (\"...\" if len(t) > n else \"\")\n",
    "\n",
    "# Group novelty rows by doc\n",
    "by_doc: Dict[str, List[Dict[str, Any]]] = {}\n",
    "for r in novelty_rows:\n",
    "    by_doc.setdefault(r[\"doc_id\"], []).append(r)\n",
    "\n",
    "# Print for a few docs\n",
    "DOCS_TO_SHOW = 3\n",
    "shown = 0\n",
    "\n",
    "for doc_id, rows in by_doc.items():\n",
    "    title = doc_info[doc_id][\"title\"]\n",
    "    rows = [r for r in rows if np.isfinite(r[\"novelty_win\"])]\n",
    "    if len(rows) < 4:\n",
    "        continue\n",
    "\n",
    "    rows_sorted = sorted(rows, key=lambda r: r[\"novelty_win\"])\n",
    "    low = rows_sorted[:2]\n",
    "    high = rows_sorted[-2:]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DOC:\", title, \"| chunks:\", len(doc_to_indices[doc_id]), \"| window:\", WINDOW)\n",
    "\n",
    "    print(\"\\nLowest novelty (most predictable):\")\n",
    "    for r in low:\n",
    "        print(f\"  win={r['novelty_win']:.3f}  idx={r['chunk_index']}  {preview_chunk(r['chunk_id'])}\")\n",
    "\n",
    "    print(\"\\nHighest novelty (most new meaning):\")\n",
    "    for r in high[::-1]:\n",
    "        print(f\"  win={r['novelty_win']:.3f}  idx={r['chunk_index']}  {preview_chunk(r['chunk_id'])}\")\n",
    "\n",
    "    shown += 1\n",
    "    if shown >= DOCS_TO_SHOW:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d97c99",
   "metadata": {},
   "source": [
    "### \"Boredom risk\" heuristic label (optional)\n",
    "\n",
    "Not a diagnosis: just a materials-side label for quick comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15de7b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_bandwidth(mean_novelty: float) -> str:\n",
    "    # Totally heuristic thresholds; tune after you see your data distribution.\n",
    "    # Lower mean novelty -> lower bandwidth -> higher boredom risk.\n",
    "    if not np.isfinite(mean_novelty):\n",
    "        return \"unknown\"\n",
    "    if mean_novelty < 0.10:\n",
    "        return \"very_low_bandwidth\"\n",
    "    if mean_novelty < 0.18:\n",
    "        return \"low_bandwidth\"\n",
    "    if mean_novelty < 0.28:\n",
    "        return \"medium_bandwidth\"\n",
    "    return \"high_bandwidth\"\n",
    "\n",
    "# Add labels to doc summaries and rewrite summary file with labels if you want.\n",
    "labeled = []\n",
    "for s in doc_summaries:\n",
    "    mean_win = s[\"novelty_win\"][\"mean\"]\n",
    "    s2 = dict(s)\n",
    "    s2[\"bandwidth_label\"] = label_bandwidth(mean_win)\n",
    "    labeled.append(s2)\n",
    "\n",
    "# Write labeled summaries (overwrite or new file)\n",
    "LABELED_OUT = OUT_DIR / \"novelty_summary_per_doc_labeled.jsonl\"\n",
    "write_jsonl(LABELED_OUT, labeled)\n",
    "\n",
    "print(\"Wrote labeled summary:\", LABELED_OUT, f\"({LABELED_OUT.stat().st_size} bytes)\")\n",
    "print(\"Example labeled row:\", labeled[0][\"title\"], labeled[0][\"novelty_win\"][\"mean\"], labeled[0][\"bandwidth_label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f02944e",
   "metadata": {},
   "source": [
    "## Next notebooks\n",
    "\n",
    "**04_redundancy_metrics.ipynb**\n",
    "- compression ratios (gzip)\n",
    "- character n-gram entropy\n",
    "- lexical repetition baselines\n",
    "\n",
    "**05_contextual_diversity.ipynb**\n",
    "- contextual diversity of words/ideas across passages\n",
    "\n",
    "Then:\n",
    "- a Streamlit app that produces a one-page \"Boredom Report\"\n",
    "  combining novelty + redundancy + diversity.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
