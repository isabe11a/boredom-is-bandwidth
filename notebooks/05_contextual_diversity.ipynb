{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d637de",
   "metadata": {},
   "source": [
    "# 05 — Contextual Diversity\n",
    "\n",
    "Contextual diversity captures how *varied* the contexts are in which words and ideas appear.\n",
    "\n",
    "Why it matters:\n",
    "- Statistical learning benefits from varied contexts\n",
    "- Over-constrained text often repeats the same frames and neighborhoods\n",
    "\n",
    "This notebook computes two complementary families of metrics:\n",
    "\n",
    "A) Lexical contextual diversity (robust, simple)\n",
    "   - document-frequency-like measures across chunks\n",
    "   - repeated-context penalties\n",
    "\n",
    "B) Semantic contextual diversity (LSA-based, more \"meaning bandwidth\")\n",
    "   - dispersion of chunk embeddings within a document\n",
    "   - neighborhood variety using cosine similarity structure\n",
    "\n",
    "Inputs:\n",
    "- `data/texts_clean/chunks.jsonl`\n",
    "- `data/lsa/chunk_embeddings.npy`\n",
    "- `data/lsa/chunk_index.jsonl`\n",
    "\n",
    "Outputs:\n",
    "- `data/diversity/word_contextual_diversity.jsonl`\n",
    "- `data/diversity/doc_contextual_diversity_summary.jsonl`\n",
    "- `data/diversity/semantic_context_dispersion_per_chunk.jsonl`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fac68f",
   "metadata": {},
   "source": [
    "## Imports + paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c40770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from _paths import set_repo_root\n",
    "ROOT = set_repo_root()\n",
    "\n",
    "CHUNKS_TEXT_IN = ROOT / \"data\" / \"texts_clean\" / \"chunks.jsonl\"\n",
    "\n",
    "EMB_IN = ROOT / \"data\" / \"lsa\" / \"chunk_embeddings.npy\"\n",
    "INDEX_IN = ROOT / \"data\" / \"lsa\" / \"chunk_index.jsonl\"\n",
    "\n",
    "OUT_DIR = ROOT / \"data\" / \"diversity\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WORD_OUT = OUT_DIR / \"word_contextual_diversity.jsonl\"\n",
    "DOC_SUMMARY_OUT = OUT_DIR / \"doc_contextual_diversity_summary.jsonl\"\n",
    "SEM_CHUNK_OUT = OUT_DIR / \"semantic_context_dispersion_per_chunk.jsonl\"\n",
    "\n",
    "print(\"Chunks:\", CHUNKS_TEXT_IN.resolve())\n",
    "print(\"Embeddings:\", EMB_IN.resolve())\n",
    "print(\"Index:\", INDEX_IN.resolve())\n",
    "print(\"Output dir:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec0632",
   "metadata": {},
   "source": [
    "## Load chunks + LSA embeddings + aligned index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5718a3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "chunks = read_jsonl(CHUNKS_TEXT_IN)\n",
    "index = read_jsonl(INDEX_IN)\n",
    "Z = np.load(EMB_IN)\n",
    "\n",
    "# Build lookup for text by chunk_id\n",
    "text_by_chunk_id = {r[\"chunk_id\"]: r[\"text\"] for r in chunks}\n",
    "\n",
    "if len(index) != Z.shape[0]:\n",
    "    raise ValueError(f\"Mismatch: {len(index)} index rows vs {Z.shape[0]} embeddings\")\n",
    "\n",
    "# Ensure index order matches embedding rows\n",
    "chunk_ids = [r[\"chunk_id\"] for r in index]\n",
    "missing_text = sum(1 for cid in chunk_ids if cid not in text_by_chunk_id)\n",
    "if missing_text:\n",
    "    print(\"Warning: missing text for\", missing_text, \"chunk_ids\")\n",
    "\n",
    "print(\"Loaded:\", len(chunk_ids), \"indexed chunks; embedding dim:\", Z.shape[1])\n",
    "print(\"Example chunk_id:\", chunk_ids[0], \"title:\", index[0][\"title\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e851df98",
   "metadata": {},
   "source": [
    "## Group by document (using index order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcdc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_rows: Dict[str, List[int]] = defaultdict(list)\n",
    "doc_info: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "for i, r in enumerate(index):\n",
    "    doc_id = r[\"doc_id\"]\n",
    "    doc_to_rows[doc_id].append(i)\n",
    "    doc_info.setdefault(doc_id, {\"title\": r[\"title\"], \"chunk_type\": r[\"chunk_type\"]})\n",
    "\n",
    "# Sort within doc by chunk_index (just to be safe)\n",
    "for doc_id, rows in doc_to_rows.items():\n",
    "    rows.sort(key=lambda i: index[i][\"chunk_index\"])\n",
    "\n",
    "print(\"Documents:\", len(doc_to_rows))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977b9242",
   "metadata": {},
   "source": [
    "## Tokenization for lexical diversity\n",
    "\n",
    "We’ll use a conservative tokenization and ignore very short tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964b37a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_TOKEN = re.compile(r\"[a-zA-Z]+(?:'[a-zA-Z]+)?|\\d+\")\n",
    "\n",
    "def tokens(text: str) -> List[str]:\n",
    "    t = text.lower()\n",
    "    toks = RE_TOKEN.findall(t)\n",
    "    # Drop tiny tokens that often act like noise in curriculum text\n",
    "    toks = [x for x in toks if len(x) >= 2]\n",
    "    return toks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22398d1b",
   "metadata": {},
   "source": [
    "## Part A — Lexical contextual diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ed675",
   "metadata": {},
   "source": [
    "### Compute word -> set(chunks) + frequency stats\n",
    "\n",
    "Definitions:\n",
    "\n",
    "- `df_chunks(word)`: in how many chunks does the word appear?\n",
    "- `contextual_diversity = df_chunks / n_chunks_in_doc (per doc)` or `/ total chunks (global)`\n",
    "- Also compute `burstiness`: how clustered the appearances are (same chunk repeated doesn’t count).\n",
    "\n",
    "We’ll compute per-doc word diversity and also a global view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7face7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_WORD_TOTAL_CT = 5      # ignore extremely rare words\n",
    "MIN_WORD_DF_CHUNKS = 2     # require appearing in >= 2 chunks for diversity stats\n",
    "\n",
    "# Global stats across all chunks\n",
    "global_word_ct = Counter()\n",
    "global_word_chunks = defaultdict(set)  # word -> {global_chunk_row_index}\n",
    "\n",
    "# Per-doc stats\n",
    "doc_word_ct = defaultdict(Counter)          # doc_id -> Counter(word)\n",
    "doc_word_chunks = defaultdict(lambda: defaultdict(set))  # doc_id -> word -> {chunk_index_in_doc}\n",
    "\n",
    "for doc_id, row_ids in doc_to_rows.items():\n",
    "    for local_pos, row_id in enumerate(row_ids):\n",
    "        cid = index[row_id][\"chunk_id\"]\n",
    "        text = text_by_chunk_id.get(cid, \"\")\n",
    "        toks = tokens(text)\n",
    "\n",
    "        # update global\n",
    "        global_word_ct.update(toks)\n",
    "        for w in set(toks):\n",
    "            global_word_chunks[w].add(row_id)\n",
    "\n",
    "        # update per-doc\n",
    "        doc_word_ct[doc_id].update(toks)\n",
    "        for w in set(toks):\n",
    "            doc_word_chunks[doc_id][w].add(local_pos)\n",
    "\n",
    "print(\"Global vocab size:\", len(global_word_ct))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb14c58f",
   "metadata": {},
   "source": [
    "### Burstiness / dispersion helper\n",
    "\n",
    "We want a simple \"are occurrences spread out or clustered?\" measure.\n",
    "\n",
    "For each word, we have the set of chunk positions it appears in: e.g., `{0,1,2,10,11}`.\n",
    "Compute mean gap between sorted positions; larger mean gap implies more dispersion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b50f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_gap(positions: List[int]) -> float:\n",
    "    if len(positions) <= 1:\n",
    "        return float(\"nan\")\n",
    "    pos = sorted(positions)\n",
    "    gaps = [pos[i+1] - pos[i] for i in range(len(pos)-1)]\n",
    "    return float(np.mean(gaps)) if gaps else float(\"nan\")\n",
    "\n",
    "def normalized_dispersion(positions: List[int], n_chunks: int) -> float:\n",
    "    \"\"\"\n",
    "    Normalize mean gap by maximum possible mean gap (~n_chunks).\n",
    "    This is a heuristic 0..1-ish score: higher = more spread out.\n",
    "    \"\"\"\n",
    "    mg = mean_gap(positions)\n",
    "    if not np.isfinite(mg) or n_chunks <= 1:\n",
    "        return float(\"nan\")\n",
    "    return float(mg / n_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71666a83",
   "metadata": {},
   "source": [
    "### Build per-word contextual diversity table (global + per doc)\n",
    "\n",
    "This writes one JSON per word per doc (plus a global summary row per word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8393f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "# Global rows\n",
    "n_global_chunks = len(index)\n",
    "for w, ct in global_word_ct.items():\n",
    "    df = len(global_word_chunks[w])\n",
    "    if ct < MIN_WORD_TOTAL_CT or df < MIN_WORD_DF_CHUNKS:\n",
    "        continue\n",
    "    word_rows.append({\n",
    "        \"scope\": \"global\",\n",
    "        \"doc_id\": None,\n",
    "        \"title\": None,\n",
    "        \"word\": w,\n",
    "        \"total_count\": int(ct),\n",
    "        \"df_chunks\": int(df),\n",
    "        \"contextual_diversity\": float(df / n_global_chunks),\n",
    "        \"dispersion_norm\": float(normalized_dispersion(list(global_word_chunks[w]), n_global_chunks)),\n",
    "    })\n",
    "\n",
    "# Per-doc rows\n",
    "for doc_id, counts in doc_word_ct.items():\n",
    "    n_chunks_doc = len(doc_to_rows[doc_id])\n",
    "    title = doc_info[doc_id][\"title\"]\n",
    "\n",
    "    for w, ct in counts.items():\n",
    "        pos_set = doc_word_chunks[doc_id].get(w, set())\n",
    "        df = len(pos_set)\n",
    "        if ct < MIN_WORD_TOTAL_CT or df < MIN_WORD_DF_CHUNKS:\n",
    "            continue\n",
    "\n",
    "        word_rows.append({\n",
    "            \"scope\": \"doc\",\n",
    "            \"doc_id\": doc_id,\n",
    "            \"title\": title,\n",
    "            \"word\": w,\n",
    "            \"total_count\": int(ct),\n",
    "            \"df_chunks\": int(df),\n",
    "            \"contextual_diversity\": float(df / n_chunks_doc),\n",
    "            \"dispersion_norm\": float(normalized_dispersion(list(pos_set), n_chunks_doc)),\n",
    "        })\n",
    "\n",
    "print(\"Word contextual diversity rows:\", len(word_rows))\n",
    "print(\"Example:\", word_rows[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592af04",
   "metadata": {},
   "source": [
    "### Save word contextual diversity JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd13ab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_jsonl(WORD_OUT, word_rows)\n",
    "print(\"Wrote:\", WORD_OUT, f\"({WORD_OUT.stat().st_size} bytes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b3230",
   "metadata": {},
   "source": [
    "## Part B — Semantic contextual diversity (LSA dispersion)\n",
    "\n",
    "Here we measure \"how many distinct semantic neighborhoods does this document cover?\" using chunk embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7e9ee0",
   "metadata": {},
   "source": [
    "### Semantic dispersion per document + per chunk\n",
    "\n",
    "Metrics:\n",
    "- `mean_pairwise_distance` (approx via sampling, to avoid O(n^2) for long docs)\n",
    "- `centroid_similarity_mean`: average cosine similarity of chunks to doc centroid (higher = more semantically tight/low variety)\n",
    "- `local_neighbor_similarity_mean`: similarity to nearest neighbors within doc (higher = more repetition)\n",
    "\n",
    "We'll compute:\n",
    "\n",
    "- per-doc summary\n",
    "- per-chunk \"semantic neighborhood tightness\" (avg similarity to k nearest chunks in same doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ff3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(v: np.ndarray, eps: float = 1e-12) -> np.ndarray:\n",
    "    n = np.linalg.norm(v)\n",
    "    if n < eps:\n",
    "        return v\n",
    "    return v / n\n",
    "\n",
    "def doc_semantic_metrics(doc_rows: List[int], Z: np.ndarray, k_nn: int = 5, sample_pairs: int = 2000) -> Dict[str, float]:\n",
    "    M = Z[doc_rows]  # [n, d] assumed normalized\n",
    "    n = M.shape[0]\n",
    "    if n <= 1:\n",
    "        return {\n",
    "            \"n_chunks\": int(n),\n",
    "            \"centroid_sim_mean\": float(\"nan\"),\n",
    "            \"centroid_sim_median\": float(\"nan\"),\n",
    "            \"mean_pairwise_distance\": float(\"nan\"),\n",
    "            \"local_nn_sim_mean\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    centroid = l2_normalize(np.mean(M, axis=0))\n",
    "    centroid_sims = M @ centroid  # cosine similarity\n",
    "    centroid_sim_mean = float(np.mean(centroid_sims))\n",
    "    centroid_sim_median = float(np.median(centroid_sims))\n",
    "\n",
    "    # Approx mean pairwise distance via random sampling of pairs\n",
    "    rng = np.random.default_rng(42)\n",
    "    pairs = min(sample_pairs, n * (n - 1) // 2)\n",
    "    if pairs <= 0:\n",
    "        mpd = float(\"nan\")\n",
    "    else:\n",
    "        i = rng.integers(0, n, size=pairs)\n",
    "        j = rng.integers(0, n, size=pairs)\n",
    "        mask = i != j\n",
    "        i, j = i[mask], j[mask]\n",
    "        sims = np.sum(M[i] * M[j], axis=1)\n",
    "        dists = 1.0 - sims\n",
    "        mpd = float(np.mean(dists)) if dists.size else float(\"nan\")\n",
    "\n",
    "    # Local nearest-neighbor similarity (within doc)\n",
    "    # Compute similarity matrix if doc small, else approximate by sampling\n",
    "    if n <= 400:\n",
    "        S = M @ M.T\n",
    "        np.fill_diagonal(S, -1.0)\n",
    "        k = min(k_nn, n - 1)\n",
    "        # average of top-k similarities for each row\n",
    "        topk = np.partition(S, -k, axis=1)[:, -k:]\n",
    "        local_nn_sim_mean = float(np.mean(topk))\n",
    "    else:\n",
    "        # Approx: sample anchor points, measure top-k within a random subset\n",
    "        anchors = rng.choice(n, size=min(80, n), replace=False)\n",
    "        k = min(k_nn, n - 1)\n",
    "        sims_all = []\n",
    "        for a in anchors:\n",
    "            candidates = rng.choice(n, size=min(300, n), replace=False)\n",
    "            candidates = candidates[candidates != a]\n",
    "            sims = M[candidates] @ M[a]\n",
    "            if sims.size >= k:\n",
    "                topk = np.partition(sims, -k)[-k:]\n",
    "                sims_all.append(np.mean(topk))\n",
    "        local_nn_sim_mean = float(np.mean(sims_all)) if sims_all else float(\"nan\")\n",
    "\n",
    "    return {\n",
    "        \"n_chunks\": int(n),\n",
    "        \"centroid_sim_mean\": centroid_sim_mean,\n",
    "        \"centroid_sim_median\": centroid_sim_median,\n",
    "        \"mean_pairwise_distance\": mpd,\n",
    "        \"local_nn_sim_mean\": local_nn_sim_mean,\n",
    "    }\n",
    "\n",
    "def per_chunk_neighbor_tightness(doc_rows: List[int], Z: np.ndarray, k_nn: int = 5) -> List[float]:\n",
    "    \"\"\"\n",
    "    For each chunk in doc, compute mean cosine similarity to its k nearest other chunks in same doc.\n",
    "    Higher = more locally repetitive / template-y semantically.\n",
    "    \"\"\"\n",
    "    M = Z[doc_rows]\n",
    "    n = M.shape[0]\n",
    "    if n <= 1:\n",
    "        return [float(\"nan\")] * n\n",
    "\n",
    "    if n <= 500:\n",
    "        S = M @ M.T\n",
    "        np.fill_diagonal(S, -1.0)\n",
    "        k = min(k_nn, n - 1)\n",
    "        topk = np.partition(S, -k, axis=1)[:, -k:]\n",
    "        return [float(np.mean(topk[i])) for i in range(n)]\n",
    "    else:\n",
    "        # Approx for very long docs\n",
    "        rng = np.random.default_rng(42)\n",
    "        k = min(k_nn, n - 1)\n",
    "        out = []\n",
    "        for i in range(n):\n",
    "            candidates = rng.choice(n, size=min(400, n), replace=False)\n",
    "            candidates = candidates[candidates != i]\n",
    "            sims = M[candidates] @ M[i]\n",
    "            if sims.size >= k:\n",
    "                topk = np.partition(sims, -k)[-k:]\n",
    "                out.append(float(np.mean(topk)))\n",
    "            else:\n",
    "                out.append(float(\"nan\"))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c64193",
   "metadata": {},
   "source": [
    "### Compute semantic dispersion per doc + per chunk outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d299d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_NN = 5\n",
    "\n",
    "doc_sem_summaries: List[Dict[str, Any]] = []\n",
    "chunk_sem_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for doc_id, row_ids in doc_to_rows.items():\n",
    "    title = doc_info[doc_id][\"title\"]\n",
    "    chunk_type = doc_info[doc_id][\"chunk_type\"]\n",
    "\n",
    "    # doc-level metrics\n",
    "    m = doc_semantic_metrics(row_ids, Z, k_nn=K_NN, sample_pairs=2000)\n",
    "    doc_sem_summaries.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"title\": title,\n",
    "        \"chunk_type\": chunk_type,\n",
    "        \"k_nn\": K_NN,\n",
    "        **m,\n",
    "        # Interpretable: higher centroid sim -> lower semantic variety\n",
    "        \"semantic_variety_proxy\": (None if not np.isfinite(m[\"mean_pairwise_distance\"]) else float(m[\"mean_pairwise_distance\"])),\n",
    "    })\n",
    "\n",
    "    # per-chunk local tightness\n",
    "    local_tight = per_chunk_neighbor_tightness(row_ids, Z, k_nn=K_NN)\n",
    "    for local_pos, row_id in enumerate(row_ids):\n",
    "        r = index[row_id]\n",
    "        chunk_sem_rows.append({\n",
    "            \"chunk_id\": r[\"chunk_id\"],\n",
    "            \"doc_id\": doc_id,\n",
    "            \"title\": title,\n",
    "            \"chunk_index\": int(r[\"chunk_index\"]),\n",
    "            \"chunk_type\": chunk_type,\n",
    "            \"k_nn\": K_NN,\n",
    "            \"local_nn_sim\": float(local_tight[local_pos]) if np.isfinite(local_tight[local_pos]) else float(\"nan\"),\n",
    "        })\n",
    "\n",
    "print(\"Doc semantic summaries:\", len(doc_sem_summaries))\n",
    "print(\"Chunk semantic rows:\", len(chunk_sem_rows))\n",
    "print(\"Example doc semantic summary:\", doc_sem_summaries[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3f2d56",
   "metadata": {},
   "source": [
    "### Save semantic per-doc summary + per-chunk semantic tightness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ea664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(DOC_SUMMARY_OUT, doc_sem_summaries)\n",
    "write_jsonl(SEM_CHUNK_OUT, chunk_sem_rows)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\"-\", DOC_SUMMARY_OUT, f\"({DOC_SUMMARY_OUT.stat().st_size} bytes)\")\n",
    "print(\"-\", SEM_CHUNK_OUT, f\"({SEM_CHUNK_OUT.stat().st_size} bytes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32f7821",
   "metadata": {},
   "source": [
    "### Fun diagnostics: docs with lowest variety (highest centroid similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc56f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort docs by centroid_sim_mean (higher = tighter semantic cluster = less variety)\n",
    "docs_sorted = sorted(\n",
    "    [d for d in doc_sem_summaries if np.isfinite(d[\"centroid_sim_mean\"])],\n",
    "    key=lambda d: -d[\"centroid_sim_mean\"]\n",
    ")\n",
    "\n",
    "print(\"\\nDocs with LOWEST semantic variety (highest centroid similarity):\")\n",
    "for d in docs_sorted[:8]:\n",
    "    print(f\"  centroid_sim_mean={d['centroid_sim_mean']:.3f}  \"\n",
    "          f\"mean_pairwise_dist={d['mean_pairwise_distance']:.3f}  \"\n",
    "          f\"local_nn_sim_mean={d['local_nn_sim_mean']:.3f}  \"\n",
    "          f\"{d['title']}  (n_chunks={d['n_chunks']})\")\n",
    "\n",
    "print(\"\\nDocs with HIGHEST semantic variety (lowest centroid similarity):\")\n",
    "for d in docs_sorted[-8:][::-1]:\n",
    "    print(f\"  centroid_sim_mean={d['centroid_sim_mean']:.3f}  \"\n",
    "          f\"mean_pairwise_dist={d['mean_pairwise_distance']:.3f}  \"\n",
    "          f\"local_nn_sim_mean={d['local_nn_sim_mean']:.3f}  \"\n",
    "          f\"{d['title']}  (n_chunks={d['n_chunks']})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050a237f",
   "metadata": {},
   "source": [
    "## Where we are\n",
    "\n",
    "You now have three complementary material-side signals:\n",
    "\n",
    "### 03 — Semantic novelty\n",
    "- new meaning introduced over time (novelty curves)\n",
    "\n",
    "### 04 — Surface redundancy\n",
    "- repetition/predictability in form (gzip, entropy, templates)\n",
    "\n",
    "### 05 — Contextual diversity\n",
    "- variety of contexts for words and ideas (lexical + semantic dispersion)\n",
    "\n",
    "## Next: unify into a \"Boredom Report\"\n",
    "\n",
    "Next step is a notebook or script that joins:\n",
    "- `data/lsa/novelty_summary_per_doc_labeled.jsonl`\n",
    "- `data/redundancy/redundancy_summary_per_doc.jsonl`\n",
    "- `data/diversity/doc_contextual_diversity_summary.jsonl`\n",
    "\n",
    "...into one table per document with:\n",
    "- semantic bandwidth score (novelty)\n",
    "- redundancy score (compression/entropy)\n",
    "- context diversity score (semantic dispersion + word DF)\n",
    "\n",
    "Then we can build the Streamlit report app.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
