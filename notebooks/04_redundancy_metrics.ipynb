{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfeb2a7a",
   "metadata": {},
   "source": [
    "# 04 — Redundancy Metrics (Surface Predictability)\n",
    "\n",
    "This notebook computes surface-form redundancy metrics for reading materials.\n",
    "\n",
    "Why:\n",
    "- Semantic novelty measures \"new meaning over time\"\n",
    "- Redundancy measures \"how repetitive / predictable the text is\"\n",
    "\n",
    "High redundancy (high compressibility, low entropy) can indicate an over-constrained text environment.\n",
    "\n",
    "Inputs:\n",
    "- `data/texts_clean/chunks.jsonl`\n",
    "\n",
    "Outputs:\n",
    "- `data/redundancy/redundancy_per_chunk.jsonl`\n",
    "- `data/redundancy/redundancy_summary_per_doc.jsonl`\n",
    "\n",
    "Note:\n",
    "These are **materials-side** metrics. They do not diagnose student boredom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2442cf20",
   "metadata": {},
   "source": [
    "## Imports + paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51325216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import gzip\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from _paths import set_repo_root\n",
    "ROOT = set_repo_root()\n",
    "\n",
    "CHUNKS_IN = ROOT / \"data\" / \"texts_clean\" / \"chunks.jsonl\"\n",
    "\n",
    "OUT_DIR = ROOT / \"data\" / \"redundancy\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PER_CHUNK_OUT = OUT_DIR / \"redundancy_per_chunk.jsonl\"\n",
    "PER_DOC_OUT = OUT_DIR / \"redundancy_summary_per_doc.jsonl\"\n",
    "\n",
    "print(\"Input:\", CHUNKS_IN.resolve())\n",
    "print(\"Output dir:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e33d665",
   "metadata": {},
   "source": [
    "## Load chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ed4a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "chunks = read_jsonl(CHUNKS_IN)\n",
    "print(\"Loaded chunks:\", len(chunks))\n",
    "\n",
    "required = {\"chunk_id\", \"doc_id\", \"title\", \"chunk_index\", \"chunk_type\", \"text\"}\n",
    "bad = [i for i, r in enumerate(chunks) if not required.issubset(r.keys())]\n",
    "if bad:\n",
    "    raise ValueError(f\"Missing required keys at row index: {bad[0]}\")\n",
    "\n",
    "# Sort deterministically\n",
    "chunks = sorted(chunks, key=lambda r: (r[\"title\"], r[\"doc_id\"], r[\"chunk_index\"], r[\"chunk_id\"]))\n",
    "print(\"Example chunk:\", chunks[0][\"title\"], \"idx\", chunks[0][\"chunk_index\"])\n",
    "print(chunks[0][\"text\"][:250])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa054ed",
   "metadata": {},
   "source": [
    "## Tokenization / normalization utilities\n",
    "\n",
    "We keep punctuation-light tokens for lexical stats. Keep a separate \"char stream\" for entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dfbcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_WS = re.compile(r\"\\s+\")\n",
    "RE_SENT_SPLIT = re.compile(r\"(?<=[.!?])\\s+\")\n",
    "RE_TOKEN = re.compile(r\"[a-zA-Z]+(?:'[a-zA-Z]+)?|\\d+\")\n",
    "\n",
    "def normalize_spaces(s: str) -> str:\n",
    "    s = s.replace(\"\\u00a0\", \" \")\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = RE_WS.sub(\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "def tokenize_words(s: str) -> List[str]:\n",
    "    s = s.lower()\n",
    "    return RE_TOKEN.findall(s)\n",
    "\n",
    "def split_sentences(s: str) -> List[str]:\n",
    "    s = normalize_spaces(s)\n",
    "    if not s:\n",
    "        return []\n",
    "    return [x.strip() for x in RE_SENT_SPLIT.split(s) if x.strip()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72135371",
   "metadata": {},
   "source": [
    "## Metric 1: gzip compression ratio\n",
    "\n",
    "Higher compression ratio -> more redundancy.\n",
    "\n",
    "We define:\n",
    "\n",
    "- `gzip_ratio = compressed_bytes / raw_bytes`\n",
    "   - Lower is \"more compressible\"; to align with \"redundancy score high = more redundant\", we define:\n",
    "\n",
    "- `redundancy_gzip = 1 - gzip_ratio` (bounded roughly 0..1-ish, not strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867cc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gzip_ratio(text: str) -> Tuple[float, int, int]:\n",
    "    raw = text.encode(\"utf-8\", errors=\"ignore\")\n",
    "    if len(raw) == 0:\n",
    "        return float(\"nan\"), 0, 0\n",
    "    comp = gzip.compress(raw)\n",
    "    return (len(comp) / len(raw)), len(comp), len(raw)\n",
    "\n",
    "def redundancy_from_gzip_ratio(r: float) -> float:\n",
    "    if not np.isfinite(r):\n",
    "        return float(\"nan\")\n",
    "    return float(1.0 - r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6232c40b",
   "metadata": {},
   "source": [
    "## Metric 2: character n-gram entropy (predictability)\n",
    "\n",
    "Compute Shannon entropy over character n-grams (default n=3). Lower entropy -> more predictable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274fe31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_ngrams(s: str, n: int = 3) -> List[str]:\n",
    "    s = normalize_spaces(s.lower())\n",
    "    # Keep spaces so we capture word-boundary predictability too\n",
    "    if len(s) < n:\n",
    "        return []\n",
    "    return [s[i:i+n] for i in range(len(s) - n + 1)]\n",
    "\n",
    "def shannon_entropy(counter: Counter) -> float:\n",
    "    total = sum(counter.values())\n",
    "    if total == 0:\n",
    "        return float(\"nan\")\n",
    "    ent = 0.0\n",
    "    for c in counter.values():\n",
    "        p = c / total\n",
    "        ent -= p * math.log2(p)\n",
    "    return ent\n",
    "\n",
    "def char_ngram_entropy(text: str, n: int = 3) -> float:\n",
    "    grams = char_ngrams(text, n=n)\n",
    "    if not grams:\n",
    "        return float(\"nan\")\n",
    "    cnt = Counter(grams)\n",
    "    return shannon_entropy(cnt)\n",
    "\n",
    "def normalize_entropy(ent: float, vocab_size: int) -> float:\n",
    "    \"\"\"\n",
    "    Normalize by maximum possible entropy log2(V).\n",
    "    Returns value in ~[0,1] if ent finite and V>1.\n",
    "    \"\"\"\n",
    "    if not np.isfinite(ent) or vocab_size <= 1:\n",
    "        return float(\"nan\")\n",
    "    return float(ent / math.log2(vocab_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd07107",
   "metadata": {},
   "source": [
    "## Metric 3: lexical repetition / diversity\n",
    "\n",
    "- Type-token ratio (TTR): unique_words / total_words (lower -> more repetitive)\n",
    "- Repeated bigram fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf20247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_stats(tokens: List[str]) -> Dict[str, float]:\n",
    "    n = len(tokens)\n",
    "    if n == 0:\n",
    "        return {\n",
    "            \"n_words\": 0,\n",
    "            \"ttr\": float(\"nan\"),\n",
    "            \"top_word_frac\": float(\"nan\"),\n",
    "            \"repeated_bigram_frac\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    counts = Counter(tokens)\n",
    "    n_types = len(counts)\n",
    "    ttr = n_types / n\n",
    "\n",
    "    top_word, top_ct = counts.most_common(1)[0]\n",
    "    top_word_frac = top_ct / n\n",
    "\n",
    "    # bigrams\n",
    "    bigrams = list(zip(tokens, tokens[1:]))\n",
    "    if len(bigrams) == 0:\n",
    "        repeated_bigram_frac = float(\"nan\")\n",
    "    else:\n",
    "        bigram_counts = Counter(bigrams)\n",
    "        repeated = sum(ct for bg, ct in bigram_counts.items() if ct >= 2)\n",
    "        repeated_bigram_frac = repeated / len(bigrams)\n",
    "\n",
    "    return {\n",
    "        \"n_words\": n,\n",
    "        \"ttr\": float(ttr),\n",
    "        \"top_word_frac\": float(top_word_frac),\n",
    "        \"repeated_bigram_frac\": float(repeated_bigram_frac),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9519525d",
   "metadata": {},
   "source": [
    "## Metric 4: \"template-y-ness\" (repeated sentence openers)\n",
    "\n",
    "This is a *delightful* redundancy signal for decodables and scripted curricula.\n",
    "\n",
    "We extract first 3–5 tokens from each sentence as an \"opener\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb0ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_openers(text: str, opener_len: int = 4) -> List[str]:\n",
    "    sents = split_sentences(text)\n",
    "    openers = []\n",
    "    for s in sents:\n",
    "        toks = tokenize_words(s)\n",
    "        if len(toks) >= 2:\n",
    "            openers.append(\" \".join(toks[:opener_len]))\n",
    "    return openers\n",
    "\n",
    "def opener_stats(openers: List[str]) -> Dict[str, float]:\n",
    "    if not openers:\n",
    "        return {\n",
    "            \"n_sentences\": 0,\n",
    "            \"opener_unique_frac\": float(\"nan\"),\n",
    "            \"top_opener_frac\": float(\"nan\"),\n",
    "        }\n",
    "    cnt = Counter(openers)\n",
    "    n = len(openers)\n",
    "    opener_unique_frac = len(cnt) / n\n",
    "    top_opener, top_ct = cnt.most_common(1)[0]\n",
    "    top_opener_frac = top_ct / n\n",
    "    return {\n",
    "        \"n_sentences\": n,\n",
    "        \"opener_unique_frac\": float(opener_unique_frac),\n",
    "        \"top_opener_frac\": float(top_opener_frac),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6145cc",
   "metadata": {},
   "source": [
    "## Compute per-chunk metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3844a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_N = 3\n",
    "OPENER_LEN = 4\n",
    "\n",
    "rows: List[Dict[str, Any]] = []\n",
    "\n",
    "for r in chunks:\n",
    "    text = r[\"text\"]\n",
    "    text_ns = normalize_spaces(text)\n",
    "\n",
    "    # gzip\n",
    "    gz_r, gz_comp, gz_raw = gzip_ratio(text_ns)\n",
    "    redundancy_gz = redundancy_from_gzip_ratio(gz_r)\n",
    "\n",
    "    # char entropy\n",
    "    grams = char_ngrams(text_ns, n=CHAR_N)\n",
    "    ent = shannon_entropy(Counter(grams)) if grams else float(\"nan\")\n",
    "    ent_norm = normalize_entropy(ent, vocab_size=len(set(grams)) if grams else 0)\n",
    "\n",
    "    # lexical\n",
    "    toks = tokenize_words(text_ns)\n",
    "    lex = lexical_stats(toks)\n",
    "\n",
    "    # openers\n",
    "    opens = sentence_openers(text_ns, opener_len=OPENER_LEN)\n",
    "    op = opener_stats(opens)\n",
    "\n",
    "    out = {\n",
    "        \"chunk_id\": r[\"chunk_id\"],\n",
    "        \"doc_id\": r[\"doc_id\"],\n",
    "        \"title\": r[\"title\"],\n",
    "        \"chunk_index\": int(r[\"chunk_index\"]),\n",
    "        \"chunk_type\": r[\"chunk_type\"],\n",
    "        \"n_chars\": len(text_ns),\n",
    "        \"gzip_ratio\": float(gz_r),\n",
    "        \"gzip_bytes_raw\": int(gz_raw),\n",
    "        \"gzip_bytes_comp\": int(gz_comp),\n",
    "        \"redundancy_gzip\": float(redundancy_gz),  # higher = more redundant\n",
    "        \"char_ngram_n\": int(CHAR_N),\n",
    "        \"char_ngram_entropy\": float(ent),\n",
    "        \"char_ngram_entropy_norm\": float(ent_norm),  # lower = more predictable\n",
    "        **lex,\n",
    "        **op,\n",
    "    }\n",
    "    rows.append(out)\n",
    "\n",
    "print(\"Computed redundancy rows:\", len(rows))\n",
    "print(\"Example:\", {k: rows[0][k] for k in [\"title\",\"chunk_index\",\"redundancy_gzip\",\"char_ngram_entropy_norm\",\"ttr\",\"top_opener_frac\"]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58c9403",
   "metadata": {},
   "source": [
    "## Save per-chunk JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63381c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(path: Path, rows: List[Dict[str, Any]]) -> None:\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_jsonl(PER_CHUNK_OUT, rows)\n",
    "print(\"Wrote:\", PER_CHUNK_OUT, f\"({PER_CHUNK_OUT.stat().st_size} bytes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251e4929",
   "metadata": {},
   "source": [
    "## Aggregate summaries per document\n",
    "\n",
    "We summarize redundancy using robust stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35735cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finite(arr: List[float]) -> np.ndarray:\n",
    "    a = np.array(arr, dtype=np.float64)\n",
    "    return a[np.isfinite(a)]\n",
    "\n",
    "def summarize(a: np.ndarray) -> Dict[str, float]:\n",
    "    if a.size == 0:\n",
    "        return {\"mean\": float(\"nan\"), \"median\": float(\"nan\"), \"p10\": float(\"nan\"), \"p90\": float(\"nan\")}\n",
    "    return {\n",
    "        \"mean\": float(np.mean(a)),\n",
    "        \"median\": float(np.median(a)),\n",
    "        \"p10\": float(np.percentile(a, 10)),\n",
    "        \"p90\": float(np.percentile(a, 90)),\n",
    "    }\n",
    "\n",
    "by_doc = defaultdict(list)\n",
    "for r in rows:\n",
    "    by_doc[r[\"doc_id\"]].append(r)\n",
    "\n",
    "doc_summaries: List[Dict[str, Any]] = []\n",
    "for doc_id, rs in by_doc.items():\n",
    "    rs = sorted(rs, key=lambda x: x[\"chunk_index\"])\n",
    "    title = rs[0][\"title\"]\n",
    "    chunk_type = rs[0][\"chunk_type\"]\n",
    "    n_chunks = len(rs)\n",
    "\n",
    "    doc_summaries.append({\n",
    "        \"doc_id\": doc_id,\n",
    "        \"title\": title,\n",
    "        \"chunk_type\": chunk_type,\n",
    "        \"n_chunks\": n_chunks,\n",
    "        \"gzip_ratio\": summarize(finite([x[\"gzip_ratio\"] for x in rs])),\n",
    "        \"redundancy_gzip\": summarize(finite([x[\"redundancy_gzip\"] for x in rs])),\n",
    "        \"char_ngram_entropy_norm\": summarize(finite([x[\"char_ngram_entropy_norm\"] for x in rs])),\n",
    "        \"ttr\": summarize(finite([x[\"ttr\"] for x in rs])),\n",
    "        \"repeated_bigram_frac\": summarize(finite([x[\"repeated_bigram_frac\"] for x in rs])),\n",
    "        \"top_opener_frac\": summarize(finite([x[\"top_opener_frac\"] for x in rs])),\n",
    "        \"opener_unique_frac\": summarize(finite([x[\"opener_unique_frac\"] for x in rs])),\n",
    "    })\n",
    "\n",
    "print(\"Doc summaries:\", len(doc_summaries))\n",
    "print(\"Example doc summary keys:\", list(doc_summaries[0].keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39ed4fa",
   "metadata": {},
   "source": [
    "## Save per-doc summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be60ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_jsonl(PER_DOC_OUT, doc_summaries)\n",
    "print(\"Wrote:\", PER_DOC_OUT, f\"({PER_DOC_OUT.stat().st_size} bytes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3432759c",
   "metadata": {},
   "source": [
    "## Fun diagnostics: most redundant chunks (by gzip) + most templated chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a84043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build lookup for preview\n",
    "text_lookup = {r[\"chunk_id\"]: r[\"text\"] for r in chunks}\n",
    "\n",
    "def preview(chunk_id: str, n: int = 220) -> str:\n",
    "    t = text_lookup.get(chunk_id, \"\").replace(\"\\n\", \" \")\n",
    "    return t[:n] + (\"...\" if len(t) > n else \"\")\n",
    "\n",
    "# Most redundant (highest redundancy_gzip)\n",
    "top_red = sorted(rows, key=lambda r: (-(r[\"redundancy_gzip\"] if np.isfinite(r[\"redundancy_gzip\"]) else -1)))[:8]\n",
    "print(\"\\nTop redundant chunks (gzip-based):\")\n",
    "for r in top_red:\n",
    "    print(f\"  red_gz={r['redundancy_gzip']:.3f}  gz_ratio={r['gzip_ratio']:.3f}  \"\n",
    "          f\"ttr={r['ttr']:.3f}  opener_top={r['top_opener_frac']:.3f}  \"\n",
    "          f\"{r['title']} idx={r['chunk_index']}  |  {preview(r['chunk_id'])}\")\n",
    "\n",
    "# Most templated (highest top_opener_frac)\n",
    "top_temp = sorted(rows, key=lambda r: (-(r[\"top_opener_frac\"] if np.isfinite(r[\"top_opener_frac\"]) else -1)))[:8]\n",
    "print(\"\\nMost templated chunks (repeated sentence openers):\")\n",
    "for r in top_temp:\n",
    "    print(f\"  opener_top={r['top_opener_frac']:.3f}  opener_unique={r['opener_unique_frac']:.3f}  \"\n",
    "          f\"{r['title']} idx={r['chunk_index']}  |  {preview(r['chunk_id'])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24951b12",
   "metadata": {},
   "source": [
    "## Next notebook: 05_contextual_diversity.ipynb\n",
    "\n",
    "We'll measure whether words/ideas appear in varied contexts across a document/corpus:\n",
    "- contextual diversity per word\n",
    "- dispersion of contexts (using LSA embeddings or co-occurrence windows)\n",
    "- \"semantic neighborhoods per 1,000 words\"\n",
    "\n",
    "Then we can unify:\n",
    "- novelty (03)\n",
    "- redundancy (04)\n",
    "- contextual diversity (05)\n",
    "\n",
    "...into a single \"Boredom Report\".\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
