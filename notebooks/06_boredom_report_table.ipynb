{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1056afc",
   "metadata": {},
   "source": [
    "# 06 — Build the Boredom Report Table (Per Document)\n",
    "\n",
    "This notebook joins outputs from:\n",
    "- 03 semantic novelty\n",
    "- 04 redundancy metrics\n",
    "- 05 contextual diversity\n",
    "\n",
    "…into a single per-document report table.\n",
    "\n",
    "Outputs:\n",
    "- `data/report/boredom_report_per_doc.jsonl`\n",
    "- `data/report/boredom_report_per_doc.csv`\n",
    "\n",
    "Important:\n",
    "This report describes properties of **materials**, not students.\n",
    "It does not diagnose boredom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f30c0ff",
   "metadata": {},
   "source": [
    "## Imports + paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d837e930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from _paths import set_repo_root\n",
    "ROOT = set_repo_root()\n",
    "\n",
    "# Inputs (from previous notebooks)\n",
    "NOVELTY_IN = ROOT / \"data\" / \"lsa\" / \"novelty_summary_per_doc_labeled.jsonl\"\n",
    "# If you didn't generate labeled, fallback to unlabeled:\n",
    "NOVELTY_FALLBACK_IN = ROOT / \"data\" / \"lsa\" / \"novelty_summary_per_doc.jsonl\"\n",
    "\n",
    "REDUNDANCY_IN = ROOT / \"data\" / \"redundancy\" / \"redundancy_summary_per_doc.jsonl\"\n",
    "DIVERSITY_IN = ROOT / \"data\" / \"diversity\" / \"doc_contextual_diversity_summary.jsonl\"\n",
    "\n",
    "OUT_DIR = ROOT / \"data\" / \"report\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_JSONL = OUT_DIR / \"boredom_report_per_doc.jsonl\"\n",
    "OUT_CSV = OUT_DIR / \"boredom_report_per_doc.csv\"\n",
    "\n",
    "print(\"Output dir:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8a19b5",
   "metadata": {},
   "source": [
    "## Read JSONL helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9694910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    if not path.exists():\n",
    "        return rows\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "nov = read_jsonl(NOVELTY_IN)\n",
    "if not nov:\n",
    "    print(\"Labeled novelty file not found; using fallback.\")\n",
    "    nov = read_jsonl(NOVELTY_FALLBACK_IN)\n",
    "\n",
    "red = read_jsonl(REDUNDANCY_IN)\n",
    "div = read_jsonl(DIVERSITY_IN)\n",
    "\n",
    "print(\"Loaded:\")\n",
    "print(\" novelty:\", len(nov))\n",
    "print(\" redundancy:\", len(red))\n",
    "print(\" diversity:\", len(div))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0e853",
   "metadata": {},
   "source": [
    "## Normalize nested dict columns into flat columns\n",
    "\n",
    "Novelty and redundancy summaries have nested stat dicts; flatten them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c155c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_stats(prefix: str, obj: Any, out: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Flatten dicts like {\"mean\":..., \"median\":..., \"p10\":..., \"p90\":...} into columns.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict) and {\"mean\",\"median\",\"p10\",\"p90\"}.issubset(obj.keys()):\n",
    "        for k, v in obj.items():\n",
    "            out[f\"{prefix}_{k}\"] = v\n",
    "    else:\n",
    "        out[prefix] = obj\n",
    "\n",
    "def flatten_row(row: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    flat: Dict[str, Any] = {}\n",
    "    for k, v in row.items():\n",
    "        if isinstance(v, dict):\n",
    "            # Try flattening stat dicts (or nested)\n",
    "            if {\"mean\",\"median\",\"p10\",\"p90\"}.issubset(v.keys()):\n",
    "                for kk, vv in v.items():\n",
    "                    flat[f\"{k}_{kk}\"] = vv\n",
    "            else:\n",
    "                # For other dicts, JSON-stringify to avoid losing info\n",
    "                flat[k] = json.dumps(v, ensure_ascii=False)\n",
    "        else:\n",
    "            flat[k] = v\n",
    "    return flat\n",
    "\n",
    "nov_df = pd.DataFrame([flatten_row(r) for r in nov])\n",
    "red_df = pd.DataFrame([flatten_row(r) for r in red])\n",
    "div_df = pd.DataFrame([flatten_row(r) for r in div])\n",
    "\n",
    "print(nov_df.columns.tolist()[:20])\n",
    "print(red_df.columns.tolist()[:20])\n",
    "print(div_df.columns.tolist()[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f544de",
   "metadata": {},
   "source": [
    "## Join into a single per-doc table\n",
    "\n",
    "We join on `doc_id`. Titles should match; we'll keep the leftmost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaa9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure doc_id exists\n",
    "for name, df in [(\"nov\", nov_df), (\"red\", red_df), (\"div\", div_df)]:\n",
    "    if \"doc_id\" not in df.columns:\n",
    "        raise ValueError(f\"{name} table missing doc_id\")\n",
    "\n",
    "df = nov_df.merge(red_df, on=[\"doc_id\"], how=\"left\", suffixes=(\"\", \"_red\"))\n",
    "df = df.merge(div_df, on=[\"doc_id\"], how=\"left\", suffixes=(\"\", \"_div\"))\n",
    "\n",
    "# Normalize title/chunk_type columns if duplicates were created\n",
    "if \"title_red\" in df.columns and \"title\" in df.columns:\n",
    "    df[\"title\"] = df[\"title\"].fillna(df[\"title_red\"])\n",
    "    df.drop(columns=[\"title_red\"], inplace=True)\n",
    "\n",
    "if \"chunk_type_red\" in df.columns and \"chunk_type\" in df.columns:\n",
    "    df[\"chunk_type\"] = df[\"chunk_type\"].fillna(df[\"chunk_type_red\"])\n",
    "    df.drop(columns=[\"chunk_type_red\"], inplace=True)\n",
    "\n",
    "print(\"Joined docs:\", len(df))\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56daff56",
   "metadata": {},
   "source": [
    "## Build composite scores (robust + interpretable)\n",
    "\n",
    "We'll define \"bandwidth\" positively (higher = better):\n",
    "\n",
    "- **Semantic bandwidth**: mean novelty (window) from 03\n",
    "- **Redundancy penalty**: redundancy_gzip_mean and entropy_norm_mean from 04\n",
    "- **Context variety**: mean_pairwise_distance and centroid_sim_mean from 05\n",
    "\n",
    "We'll rank-normalize (percentiles) so scales don't dominate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f4aedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_rank(series: pd.Series) -> pd.Series:\n",
    "    # returns values in [0,1], NaNs preserved\n",
    "    s = series.copy()\n",
    "    mask = s.notna()\n",
    "    s2 = s[mask].rank(pct=True)\n",
    "    out = pd.Series(np.nan, index=s.index)\n",
    "    out[mask] = s2\n",
    "    return out\n",
    "\n",
    "# --- Pick key columns (fallbacks included) ---\n",
    "# Novelty: prefer novelty_win_mean\n",
    "nov_col = \"novelty_win_mean\" if \"novelty_win_mean\" in df.columns else \"novelty_cum_mean\"\n",
    "\n",
    "# Redundancy: higher redundancy_gzip_mean = more redundant (bad)\n",
    "red_gz_col = \"redundancy_gzip_mean\" if \"redundancy_gzip_mean\" in df.columns else None\n",
    "# Entropy norm: lower entropy = more predictable (bad), so we want higher entropy as better\n",
    "ent_col = \"char_ngram_entropy_norm_mean\" if \"char_ngram_entropy_norm_mean\" in df.columns else None\n",
    "\n",
    "# Semantic variety: mean_pairwise_distance higher = more variety (good)\n",
    "mpd_col = \"mean_pairwise_distance\" if \"mean_pairwise_distance\" in df.columns else None\n",
    "# Centroid similarity higher = tighter cluster (bad), so invert for variety\n",
    "cent_col = \"centroid_sim_mean\" if \"centroid_sim_mean\" in df.columns else None\n",
    "\n",
    "# Create ranked components\n",
    "df[\"rank_semantic_novelty\"] = percentile_rank(df[nov_col]) if nov_col else np.nan\n",
    "df[\"rank_entropy_good\"] = percentile_rank(df[ent_col]) if ent_col else np.nan\n",
    "\n",
    "if red_gz_col:\n",
    "    df[\"rank_redundancy_bad\"] = percentile_rank(df[red_gz_col])  # higher = worse\n",
    "else:\n",
    "    df[\"rank_redundancy_bad\"] = np.nan\n",
    "\n",
    "if mpd_col:\n",
    "    df[\"rank_semantic_variety\"] = percentile_rank(df[mpd_col])  # higher = better\n",
    "else:\n",
    "    df[\"rank_semantic_variety\"] = np.nan\n",
    "\n",
    "if cent_col:\n",
    "    df[\"rank_centroid_variety\"] = 1.0 - percentile_rank(df[cent_col])  # invert (higher centroid sim -> lower variety)\n",
    "else:\n",
    "    df[\"rank_centroid_variety\"] = np.nan\n",
    "\n",
    "# Combine variety from mpd + centroid-based if both exist\n",
    "var_parts = [\"rank_semantic_variety\", \"rank_centroid_variety\"]\n",
    "df[\"rank_variety_combined\"] = df[var_parts].mean(axis=1, skipna=True)\n",
    "\n",
    "# Composite bandwidth score: encourage novelty + variety + entropy, penalize redundancy\n",
    "# We subtract redundancy_bad with a small weight (to avoid double-counting with entropy).\n",
    "df[\"bandwidth_score\"] = (\n",
    "    0.40 * df[\"rank_semantic_novelty\"] +\n",
    "    0.30 * df[\"rank_variety_combined\"] +\n",
    "    0.20 * df[\"rank_entropy_good\"] -\n",
    "    0.10 * df[\"rank_redundancy_bad\"]\n",
    ")\n",
    "\n",
    "# Convenience: \"boredom risk\" as inverse of bandwidth score (materials-side only)\n",
    "df[\"boredom_risk_proxy\"] = 1.0 - df[\"bandwidth_score\"]\n",
    "\n",
    "# Make a friendly bucket label\n",
    "def bucket(x: float) -> str:\n",
    "    if not np.isfinite(x):\n",
    "        return \"unknown\"\n",
    "    if x < 0.25:\n",
    "        return \"very_low_bandwidth\"\n",
    "    if x < 0.45:\n",
    "        return \"low_bandwidth\"\n",
    "    if x < 0.65:\n",
    "        return \"medium_bandwidth\"\n",
    "    return \"high_bandwidth\"\n",
    "\n",
    "df[\"bandwidth_bucket\"] = df[\"bandwidth_score\"].apply(bucket)\n",
    "\n",
    "print(\"Composite computed. Non-NaN bandwidth_score:\", df[\"bandwidth_score\"].notna().sum())\n",
    "df[[\"title\", nov_col, \"bandwidth_score\", \"bandwidth_bucket\"]].head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8ffaac",
   "metadata": {},
   "source": [
    "## Save report table (CSV + JSONL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a13715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by bandwidth_score descending\n",
    "df_sorted = df.sort_values(by=\"bandwidth_score\", ascending=False)\n",
    "\n",
    "# Save CSV\n",
    "df_sorted.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "# Save JSONL\n",
    "with OUT_JSONL.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        rec = row.to_dict()\n",
    "        # Convert numpy types to Python types for JSON\n",
    "        for k, v in list(rec.items()):\n",
    "            if isinstance(v, (np.integer,)):\n",
    "                rec[k] = int(v)\n",
    "            elif isinstance(v, (np.floating,)):\n",
    "                rec[k] = float(v)\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\"-\", OUT_CSV, f\"({OUT_CSV.stat().st_size} bytes)\")\n",
    "print(\"-\", OUT_JSONL, f\"({OUT_JSONL.stat().st_size} bytes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b21acfa",
   "metadata": {},
   "source": [
    "## Fun printouts: \"most over-constrained\" vs \"highest bandwidth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985cac76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top(df: pd.DataFrame, n: int = 8, ascending: bool = False, label: str = \"\"):\n",
    "    d = df.sort_values(\"bandwidth_score\", ascending=ascending).head(n)\n",
    "    print(\"\\n\" + (\"=\"*80))\n",
    "    print(label)\n",
    "    for _, r in d.iterrows():\n",
    "        print(f\"  score={r['bandwidth_score']:.3f}  bucket={r['bandwidth_bucket']:<18}  title={r.get('title','')}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "show_top(df, n=10, ascending=True, label=\"Most over-constrained (lowest bandwidth) — materials-side proxy\")\n",
    "show_top(df, n=10, ascending=False, label=\"Highest bandwidth (most semantic variety / novelty) — materials-side proxy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3cd711",
   "metadata": {},
   "source": [
    "## Quick sanity: correlate components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0cde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"rank_semantic_novelty\",\"rank_variety_combined\",\"rank_entropy_good\",\"rank_redundancy_bad\",\"bandwidth_score\"]\n",
    "corr = df[cols].corr(numeric_only=True)\n",
    "corr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c9e89c",
   "metadata": {},
   "source": [
    "## Next: Streamlit app\n",
    "\n",
    "Next we’ll build `app/streamlit_app.py` that:\n",
    "- lets you upload text sets\n",
    "- runs notebooks’ logic (or loads precomputed report files)\n",
    "- renders a one-page “Boredom Report” with:\n",
    "  - bandwidth bucket + score\n",
    "  - novelty curve (if available)\n",
    "  - redundancy highlights (“most templated chunk”)\n",
    "  - context diversity summary\n",
    "\n",
    "If you want the app to be *fast*, we’ll start by reading the precomputed\n",
    "`data/report/boredom_report_per_doc.csv` and adding optional drill-down views.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
