{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f355037b",
   "metadata": {},
   "source": [
    "# 01 — Ingest & Clean\n",
    "\n",
    "This notebook:\n",
    "1) Ingests reading materials from:\n",
    "   - pasted text\n",
    "   - `.txt` files\n",
    "   - `.docx` files  \n",
    "2) Cleans common artifacts (headers/footers, extra whitespace, weird punctuation)\n",
    "3) Chunks text into analysis units (paragraphs or pseudo-pages)\n",
    "4) Saves standardized JSONL outputs for downstream metrics.\n",
    "\n",
    "Outputs:\n",
    "- `data/texts_clean/documents.jsonl`\n",
    "- `data/texts_clean/chunks.jsonl`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafee8eb",
   "metadata": {},
   "source": [
    "## Imports + paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89089bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "import hashlib\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Optional, Iterable, Tuple\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "while ROOT != ROOT.parent and not (ROOT / \".git\").exists():\n",
    "    ROOT = ROOT.parent\n",
    "\n",
    "os.chdir(ROOT)\n",
    "\n",
    "DATA_RAW = ROOT / \"data\" / \"texts_raw\"\n",
    "DATA_CLEAN = ROOT / \"data\" / \"texts_clean\"\n",
    "\n",
    "print(\"Repo root:\", ROOT)\n",
    "print(\"Raw:\", DATA_RAW, \"exists?\", DATA_RAW.exists())\n",
    "print(\"Clean:\", DATA_CLEAN, \"exists?\", DATA_CLEAN.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3901e5",
   "metadata": {},
   "source": [
    "### Optional docx support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf4c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCX_AVAILABLE = False\n",
    "try:\n",
    "    import docx  # python-docx\n",
    "    DOCX_AVAILABLE = True\n",
    "except Exception as e:\n",
    "    DOCX_AVAILABLE = False\n",
    "    print(\"python-docx not available. .docx ingestion will be skipped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b88c2",
   "metadata": {},
   "source": [
    "## Data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6c8baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Document:\n",
    "    doc_id: str\n",
    "    title: str\n",
    "    source_path: str\n",
    "    raw_text: str\n",
    "    clean_text: str\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    title: str\n",
    "    chunk_index: int\n",
    "    chunk_type: str  # \"paragraph\" or \"page\"\n",
    "    text: str\n",
    "    n_chars: int\n",
    "    n_tokens_approx: int\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd40526",
   "metadata": {},
   "source": [
    "## Utility: stable IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9136625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_id(*parts: str) -> str:\n",
    "    h = hashlib.sha1()\n",
    "    for p in parts:\n",
    "        h.update(p.encode(\"utf-8\", errors=\"ignore\"))\n",
    "        h.update(b\"|\")\n",
    "    return h.hexdigest()[:12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8886894b",
   "metadata": {},
   "source": [
    "## Ingestion: read txt/docx + optional pasted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc4aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(path: Path) -> str:\n",
    "    return path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def read_docx(path: Path) -> str:\n",
    "    if not DOCX_AVAILABLE:\n",
    "        raise RuntimeError(\"python-docx not installed\")\n",
    "    d = docx.Document(str(path))\n",
    "    # Keep paragraph boundaries; join with double newline\n",
    "    paras = [p.text for p in d.paragraphs if p.text is not None]\n",
    "    return \"\\n\\n\".join([p for p in paras if p.strip()])\n",
    "\n",
    "def ingest_from_folder(folder: Path) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"\n",
    "    Returns list of (title, source_path, raw_text).\n",
    "    \"\"\"\n",
    "    items: List[Tuple[str, str, str]] = []\n",
    "    if not folder.exists():\n",
    "        print(f\"Folder does not exist: {folder}\")\n",
    "        return items\n",
    "\n",
    "    # TXT\n",
    "    for fp in sorted(folder.glob(\"*.txt\")):\n",
    "        items.append((fp.stem, str(fp), read_txt(fp)))\n",
    "\n",
    "    # DOCX\n",
    "    if DOCX_AVAILABLE:\n",
    "        for fp in sorted(folder.glob(\"*.docx\")):\n",
    "            items.append((fp.stem, str(fp), read_docx(fp)))\n",
    "\n",
    "    return items\n",
    "\n",
    "# Optional: paste text here for quick experiments\n",
    "PASTED_TITLE = \"\"  # e.g., \"Decodable_Set_A\"\n",
    "PASTED_TEXT = \"\"   # paste text here\n",
    "\n",
    "raw_items = ingest_from_folder(DATA_RAW)\n",
    "\n",
    "if PASTED_TEXT.strip():\n",
    "    raw_items.append((PASTED_TITLE or \"pasted_text\", \"pasted://\", PASTED_TEXT))\n",
    "\n",
    "print(f\"Ingested {len(raw_items)} item(s).\")\n",
    "for t, p, _ in raw_items[:5]:\n",
    "    print(\"-\", t, \"(\", p, \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8017f",
   "metadata": {},
   "source": [
    "## Cleaning functions (curriculum-friendly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77795120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common junk patterns:\n",
    "RE_MULTISPACE = re.compile(r\"[ \\t]+\")\n",
    "RE_MULTI_NL = re.compile(r\"\\n{3,}\")\n",
    "RE_SOFT_HYPHEN = re.compile(\"\\u00ad\")  # discretionary hyphen\n",
    "RE_NONBREAKING_SPACE = re.compile(\"\\u00a0\")\n",
    "\n",
    "# Simple header/footer removal heuristics:\n",
    "# - lines that are mostly digits (page numbers)\n",
    "# - lines that repeat frequently\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    s = RE_NONBREAKING_SPACE.sub(\" \", s)\n",
    "    s = RE_SOFT_HYPHEN.sub(\"\", s)\n",
    "\n",
    "    # Normalize quotes/dashes a bit (optional)\n",
    "    s = s.replace(\"“\", '\"').replace(\"”\", '\"').replace(\"’\", \"'\").replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "\n",
    "    # Collapse spaces, normalize newlines\n",
    "    s = RE_MULTISPACE.sub(\" \", s)\n",
    "    s = RE_MULTI_NL.sub(\"\\n\\n\", s)\n",
    "    return s.strip()\n",
    "\n",
    "def split_lines(s: str) -> List[str]:\n",
    "    return [ln.strip() for ln in s.split(\"\\n\")]\n",
    "\n",
    "def detect_repeated_lines(lines: List[str], min_len: int = 8, min_count: int = 3) -> set:\n",
    "    \"\"\"\n",
    "    Finds lines that repeat often (typical headers/footers).\n",
    "    Conservative: only longer lines repeated >= min_count.\n",
    "    \"\"\"\n",
    "    freq: Dict[str, int] = {}\n",
    "    for ln in lines:\n",
    "        key = ln.strip()\n",
    "        if len(key) < min_len:\n",
    "            continue\n",
    "        freq[key] = freq.get(key, 0) + 1\n",
    "    return {k for k, v in freq.items() if v >= min_count}\n",
    "\n",
    "def drop_headers_footers(s: str) -> str:\n",
    "    lines = split_lines(s)\n",
    "    repeated = detect_repeated_lines(lines)\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for ln in lines:\n",
    "        if not ln:\n",
    "            cleaned_lines.append(\"\")\n",
    "            continue\n",
    "\n",
    "        # Drop pure page numbers or \"Page 3\" style\n",
    "        if re.fullmatch(r\"\\d{1,3}\", ln):\n",
    "            continue\n",
    "        if re.fullmatch(r\"(page|pg)\\s*\\d{1,3}\", ln.lower()):\n",
    "            continue\n",
    "\n",
    "        # Drop repeated header/footer lines\n",
    "        if ln in repeated:\n",
    "            continue\n",
    "\n",
    "        cleaned_lines.append(ln)\n",
    "\n",
    "    out = \"\\n\".join(cleaned_lines)\n",
    "    out = RE_MULTI_NL.sub(\"\\n\\n\", out)\n",
    "    return out.strip()\n",
    "\n",
    "def clean_document_text(raw: str) -> str:\n",
    "    s = normalize_text(raw)\n",
    "    s = drop_headers_footers(s)\n",
    "    s = normalize_text(s)  # re-normalize after dropping lines\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f6a196",
   "metadata": {},
   "source": [
    "## Chunking: paragraphs or pseudo-pages\n",
    "\n",
    "This gives you two options:\n",
    "\n",
    "- **paragraph chunks** (best default)\n",
    "- **pseudo-pages** (approximate \"page\" by character budget; useful for novelty curves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_token_count(text: str) -> int:\n",
    "    # quick approximation good enough for reporting\n",
    "    return max(1, len(text.split()))\n",
    "\n",
    "def chunk_by_paragraph(text: str) -> List[str]:\n",
    "    paras = [p.strip() for p in text.split(\"\\n\\n\")]\n",
    "    paras = [p for p in paras if p]\n",
    "    return paras\n",
    "\n",
    "def chunk_by_pseudopage(text: str, target_chars: int = 1200, min_chars: int = 500) -> List[str]:\n",
    "    \"\"\"\n",
    "    Greedy accumulation of paragraphs into ~page-sized chunks.\n",
    "    \"\"\"\n",
    "    paras = chunk_by_paragraph(text)\n",
    "    pages = []\n",
    "    buf = []\n",
    "    buf_len = 0\n",
    "\n",
    "    for p in paras:\n",
    "        if buf_len + len(p) + 2 <= target_chars or buf_len < min_chars:\n",
    "            buf.append(p)\n",
    "            buf_len += len(p) + 2\n",
    "        else:\n",
    "            pages.append(\"\\n\\n\".join(buf).strip())\n",
    "            buf = [p]\n",
    "            buf_len = len(p)\n",
    "\n",
    "    if buf:\n",
    "        pages.append(\"\\n\\n\".join(buf).strip())\n",
    "\n",
    "    return [pg for pg in pages if pg]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17505e4",
   "metadata": {},
   "source": [
    "### Build documents + chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45170897",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents: List[Document] = []\n",
    "chunks: List[Chunk] = []\n",
    "\n",
    "CHUNK_MODE = \"paragraph\"  # \"paragraph\" or \"page\"\n",
    "PSEUDOPAGE_TARGET_CHARS = 1200\n",
    "\n",
    "for title, source_path, raw_text in raw_items:\n",
    "    clean_text = clean_document_text(raw_text)\n",
    "\n",
    "    doc_id = stable_id(title, source_path, clean_text[:200])\n",
    "    documents.append(Document(\n",
    "        doc_id=doc_id,\n",
    "        title=title,\n",
    "        source_path=source_path,\n",
    "        raw_text=raw_text,\n",
    "        clean_text=clean_text\n",
    "    ))\n",
    "\n",
    "    if CHUNK_MODE == \"paragraph\":\n",
    "        unit_texts = chunk_by_paragraph(clean_text)\n",
    "        chunk_type = \"paragraph\"\n",
    "    elif CHUNK_MODE == \"page\":\n",
    "        unit_texts = chunk_by_pseudopage(clean_text, target_chars=PSEUDOPAGE_TARGET_CHARS)\n",
    "        chunk_type = \"page\"\n",
    "    else:\n",
    "        raise ValueError(\"CHUNK_MODE must be 'paragraph' or 'page'\")\n",
    "\n",
    "    for i, t in enumerate(unit_texts):\n",
    "        chunk_id = stable_id(doc_id, str(i), t[:200])\n",
    "        chunks.append(Chunk(\n",
    "            chunk_id=chunk_id,\n",
    "            doc_id=doc_id,\n",
    "            title=title,\n",
    "            chunk_index=i,\n",
    "            chunk_type=chunk_type,\n",
    "            text=t,\n",
    "            n_chars=len(t),\n",
    "            n_tokens_approx=approx_token_count(t),\n",
    "        ))\n",
    "\n",
    "print(f\"Documents: {len(documents)}\")\n",
    "print(f\"Chunks: {len(chunks)} (mode={CHUNK_MODE})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c396248",
   "metadata": {},
   "source": [
    "### Quick sanity checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0fc523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preview(doc: Document, n_chars: int = 800):\n",
    "    print(\"TITLE:\", doc.title)\n",
    "    print(\"DOC_ID:\", doc.doc_id)\n",
    "    print(\"SOURCE:\", doc.source_path)\n",
    "    print(\"\\nCLEAN PREVIEW:\\n\")\n",
    "    print(doc.clean_text[:n_chars])\n",
    "    print(\"\\n---\\n\")\n",
    "\n",
    "if documents:\n",
    "    preview(documents[0])\n",
    "\n",
    "# Chunk stats\n",
    "if chunks:\n",
    "    lens = [c.n_tokens_approx for c in chunks]\n",
    "    print(\"Chunk token approx:\")\n",
    "    print(\" min:\", min(lens))\n",
    "    print(\" p50:\", sorted(lens)[len(lens)//2])\n",
    "    print(\" max:\", max(lens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674bf2f0",
   "metadata": {},
   "source": [
    "## Save JSONL outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508a5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(path: Path, rows: Iterable[dict]) -> None:\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for r in rows:\n",
    "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "write_jsonl(DOCS_OUT, (asdict(d) for d in documents))\n",
    "write_jsonl(CHUNKS_OUT, (asdict(c) for c in chunks))\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\"-\", DOCS_OUT, f\"({DOCS_OUT.stat().st_size} bytes)\")\n",
    "print(\"-\", CHUNKS_OUT, f\"({CHUNKS_OUT.stat().st_size} bytes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a87c5be",
   "metadata": {},
   "source": [
    "## Next notebook\n",
    "\n",
    "Move on to **02_train_lsa.ipynb**, which will:\n",
    "- load `chunks.jsonl`\n",
    "- train LSA/LSI representations\n",
    "- save embeddings for downstream novelty / redundancy / diversity metrics\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
