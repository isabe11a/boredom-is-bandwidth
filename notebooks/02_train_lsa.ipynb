{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60989f7",
   "metadata": {},
   "source": [
    "# 02 — Train LSA / LSI Embeddings\n",
    "\n",
    "This notebook:\n",
    "1) Loads standardized chunks from `data/texts_clean/chunks.jsonl`\n",
    "2) Builds a TF-IDF matrix\n",
    "3) Fits TruncatedSVD to learn an LSA/LSI semantic space\n",
    "4) Produces an embedding vector for every chunk\n",
    "5) Saves the model + embeddings for downstream novelty metrics\n",
    "\n",
    "Outputs:\n",
    "- `data/lsa/model.joblib`\n",
    "- `data/lsa/chunk_embeddings.npy`\n",
    "- `data/lsa/chunk_index.jsonl`\n",
    "- `data/lsa/nearest_neighbors.jsonl` (optional)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdac50a",
   "metadata": {},
   "source": [
    "## Imports + paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import joblib\n",
    "\n",
    "from _paths import set_repo_root\n",
    "ROOT = set_repo_root()\n",
    "\n",
    "CHUNKS_IN = ROOT / \"data\" / \"texts_clean\" / \"chunks.jsonl\"\n",
    "\n",
    "OUT_DIR = ROOT / \"data\" / \"lsa\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_OUT = OUT_DIR / \"model.joblib\"\n",
    "EMB_OUT = OUT_DIR / \"chunk_embeddings.npy\"\n",
    "INDEX_OUT = OUT_DIR / \"chunk_index.jsonl\"\n",
    "NN_OUT = OUT_DIR / \"nearest_neighbors.jsonl\"\n",
    "\n",
    "print(\"Input:\", CHUNKS_IN.resolve())\n",
    "print(\"Output dir:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3db670e",
   "metadata": {},
   "source": [
    "## Load chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf5090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path: Path) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "chunks = read_jsonl(CHUNKS_IN)\n",
    "print(\"Loaded chunks:\", len(chunks))\n",
    "\n",
    "# Minimal validation\n",
    "required = {\"chunk_id\", \"doc_id\", \"title\", \"chunk_index\", \"chunk_type\", \"text\"}\n",
    "missing = [i for i, r in enumerate(chunks) if not required.issubset(r.keys())]\n",
    "if missing:\n",
    "    raise ValueError(f\"Some rows are missing required keys. Example bad row index: {missing[0]}\")\n",
    "\n",
    "# Sort deterministically: by title then chunk_index (within doc) then chunk_id\n",
    "chunks = sorted(chunks, key=lambda r: (r[\"title\"], r[\"doc_id\"], r[\"chunk_index\"], r[\"chunk_id\"]))\n",
    "texts = [r[\"text\"] for r in chunks]\n",
    "print(\"Example chunk:\\n\", texts[0][:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59369fe0",
   "metadata": {},
   "source": [
    "## Text normalization for vectorization\n",
    "\n",
    "We keep this light: lowercasing, stripping, collapsing whitespace. Don't over-clean (LSA wants distributional signal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca0e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_WS = re.compile(r\"\\s+\")\n",
    "RE_NONWORD_SPACE = re.compile(r\"[^a-zA-Z0-9'\\- ]+\")\n",
    "\n",
    "def normalize_for_tfidf(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = s.replace(\"\\u00a0\", \" \")  # NBSP\n",
    "    s = RE_WS.sub(\" \", s)\n",
    "    # Keep apostrophes and hyphens; drop other punctuation to reduce feature sparsity\n",
    "    s = RE_NONWORD_SPACE.sub(\" \", s)\n",
    "    s = RE_WS.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "texts_norm = [normalize_for_tfidf(t) for t in texts]\n",
    "print(\"Normalized example:\\n\", texts_norm[0][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f354f7a",
   "metadata": {},
   "source": [
    "## Configure TF-IDF + SVD\n",
    "\n",
    "These defaults are good for small-to-medium corpora. Tune later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c02904e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TF-IDF settings ---\n",
    "# min_df: ignore very rare terms (noise)\n",
    "# max_df: ignore extremely common terms (near stopwords)\n",
    "# ngram_range: (1,2) helps capture short phrases without going wild\n",
    "TFIDF_MIN_DF = 2\n",
    "TFIDF_MAX_DF = 0.9\n",
    "NGRAM_RANGE = (1, 2)\n",
    "MAX_FEATURES = 50000  # cap for stability\n",
    "\n",
    "# --- SVD settings ---\n",
    "N_COMPONENTS = 200  # 100–400 typical; depends on corpus size\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"TF-IDF:\", dict(min_df=TFIDF_MIN_DF, max_df=TFIDF_MAX_DF, ngram_range=NGRAM_RANGE, max_features=MAX_FEATURES))\n",
    "print(\"SVD:\", dict(n_components=N_COMPONENTS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d82ed",
   "metadata": {},
   "source": [
    "## Fit TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8b5e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    min_df=TFIDF_MIN_DF,\n",
    "    max_df=TFIDF_MAX_DF,\n",
    "    ngram_range=NGRAM_RANGE,\n",
    "    max_features=MAX_FEATURES,\n",
    "    strip_accents=\"unicode\",\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(texts_norm)\n",
    "print(\"TF-IDF shape:\", X_tfidf.shape)\n",
    "\n",
    "# Quick feature sanity check\n",
    "feat_names = vectorizer.get_feature_names_out()\n",
    "print(\"Num features:\", len(feat_names))\n",
    "print(\"Sample features:\", feat_names[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d7882a",
   "metadata": {},
   "source": [
    "## Fit LSA (TruncatedSVD) and create embeddings\n",
    "\n",
    "Classic LSA uses SVD on TF-IDF (or term counts). We normalize embeddings to unit length for cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4020fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=min(N_COMPONENTS, X_tfidf.shape[1]-1), random_state=RANDOM_STATE)\n",
    "Z = svd.fit_transform(X_tfidf)  # [n_chunks, n_components]\n",
    "\n",
    "# Normalize to unit length (so dot product ~ cosine similarity)\n",
    "Z = normalize(Z, norm=\"l2\")\n",
    "\n",
    "explained = float(np.sum(svd.explained_variance_ratio_))\n",
    "print(\"Embeddings shape:\", Z.shape)\n",
    "print(\"Explained variance ratio (sum):\", round(explained, 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcffcdc5",
   "metadata": {},
   "source": [
    "## Save model + embeddings + index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e99e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model as a dict so you can load everything cleanly\n",
    "model = {\n",
    "    \"vectorizer\": vectorizer,\n",
    "    \"svd\": svd,\n",
    "    \"settings\": {\n",
    "        \"tfidf_min_df\": TFIDF_MIN_DF,\n",
    "        \"tfidf_max_df\": TFIDF_MAX_DF,\n",
    "        \"ngram_range\": NGRAM_RANGE,\n",
    "        \"max_features\": MAX_FEATURES,\n",
    "        \"n_components\": int(Z.shape[1]),\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    },\n",
    "}\n",
    "\n",
    "joblib.dump(model, MODEL_OUT)\n",
    "np.save(EMB_OUT, Z)\n",
    "\n",
    "# Save aligned metadata index (one JSON per row, same row order as embeddings)\n",
    "with INDEX_OUT.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for r in chunks:\n",
    "        meta = {\n",
    "            \"chunk_id\": r[\"chunk_id\"],\n",
    "            \"doc_id\": r[\"doc_id\"],\n",
    "            \"title\": r[\"title\"],\n",
    "            \"chunk_index\": r[\"chunk_index\"],\n",
    "            \"chunk_type\": r[\"chunk_type\"],\n",
    "            \"n_chars\": r.get(\"n_chars\", None),\n",
    "            \"n_tokens_approx\": r.get(\"n_tokens_approx\", None),\n",
    "        }\n",
    "        f.write(json.dumps(meta, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"-\", MODEL_OUT, f\"({MODEL_OUT.stat().st_size} bytes)\")\n",
    "print(\"-\", EMB_OUT, f\"({EMB_OUT.stat().st_size} bytes)\")\n",
    "print(\"-\", INDEX_OUT, f\"({INDEX_OUT.stat().st_size} bytes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4498fc",
   "metadata": {},
   "source": [
    "### Quick qualitative check: nearest neighbors (optional but fun)\n",
    "\n",
    "This helps validate that the semantic space is doing something reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_neighbors(Z: np.ndarray, idx: int, k: int = 5) -> List[Tuple[int, float]]:\n",
    "    sims = (Z @ Z[idx])  # cosine similarity because Z normalized\n",
    "    sims[idx] = -1.0\n",
    "    nn = np.argsort(-sims)[:k]\n",
    "    return [(int(i), float(sims[i])) for i in nn]\n",
    "\n",
    "# Pick a few example indices: first chunk of each document\n",
    "example_indices = []\n",
    "seen_docs = set()\n",
    "for i, r in enumerate(chunks):\n",
    "    if r[\"doc_id\"] not in seen_docs:\n",
    "        seen_docs.add(r[\"doc_id\"])\n",
    "        example_indices.append(i)\n",
    "    if len(example_indices) >= 5:\n",
    "        break\n",
    "\n",
    "for idx in example_indices:\n",
    "    r = chunks[idx]\n",
    "    print(\"\\nQUERY:\", r[\"title\"], \"chunk\", r[\"chunk_index\"])\n",
    "    print(\"TEXT:\", r[\"text\"][:200].replace(\"\\n\", \" \") + (\"...\" if len(r[\"text\"])>200 else \"\"))\n",
    "    for j, s in top_neighbors(Z, idx, k=5):\n",
    "        rr = chunks[j]\n",
    "        print(\"  NN:\", f\"{s:.3f}\", \"|\", rr[\"title\"], \"chunk\", rr[\"chunk_index\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662ae5fa",
   "metadata": {},
   "source": [
    "### Save neighbors to JSONL (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_NEIGHBORS = True\n",
    "K = 5\n",
    "\n",
    "if SAVE_NEIGHBORS:\n",
    "    with NN_OUT.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for i, r in enumerate(chunks):\n",
    "            nns = top_neighbors(Z, i, k=K)\n",
    "            out = {\n",
    "                \"chunk_id\": r[\"chunk_id\"],\n",
    "                \"title\": r[\"title\"],\n",
    "                \"chunk_index\": r[\"chunk_index\"],\n",
    "                \"neighbors\": [\n",
    "                    {\n",
    "                        \"neighbor_chunk_id\": chunks[j][\"chunk_id\"],\n",
    "                        \"neighbor_title\": chunks[j][\"title\"],\n",
    "                        \"neighbor_chunk_index\": chunks[j][\"chunk_index\"],\n",
    "                        \"cosine_sim\": s,\n",
    "                    }\n",
    "                    for j, s in nns\n",
    "                ],\n",
    "            }\n",
    "            f.write(json.dumps(out, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(\"Wrote neighbors:\", NN_OUT, f\"({NN_OUT.stat().st_size} bytes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8453d2",
   "metadata": {},
   "source": [
    "## Next notebook: 03_semantic_novelty.ipynb\n",
    "\n",
    "We now have normalized LSA embeddings `chunk_embeddings.npy` aligned with `chunk_index.jsonl`.\n",
    "\n",
    "Next we’ll compute novelty curves, e.g.:\n",
    "\n",
    "- per document:\n",
    "  - novelty of chunk t = 1 - cos( embedding(t), centroid(0..t-1) )\n",
    "  - or novelty relative to a trailing window (e.g. last 5 chunks)\n",
    "\n",
    "Then we can compare:\n",
    "- decodable vs authentic sets\n",
    "- curriculum A vs curriculum B\n",
    "- \"semantic bandwidth\" over time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
